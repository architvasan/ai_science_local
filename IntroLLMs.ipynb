{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM2N0IJ8niJmpAEBhx5iFjj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/architvasan/ai_science_local/blob/main/IntroLLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Language models (LMs)\n",
        "\n",
        "Author: Archit Vasan , including materials on LLMs by Varuni Sastri and Carlo Graziani at Argonne, and discussion/editorial work by Taylor Childers, Bethany Lusch, and Venkat Vishwanath (Argonne)\n",
        "\n",
        "Inspiration from the blog posts \"The Illustrated Transformer\" and \"The Illustrated GPT2\" by Jay Alammar, highly recommended reading.\n",
        "\n",
        "Although the name \"language models\" is derived from Natural Language Processing, the models used in these approaches can be applied to diverse scientific applications as illustrated below.\n",
        "\n",
        "This session is dedicated to setting out the basics of sequential data modeling, and introducing a few key elements required for DL approaches to such modeling---principally Transformers."
      ],
      "metadata": {
        "id": "FU00jtHMWQa9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "During this session I will cover:\n",
        "1. Scientific applications for language models\n",
        "2. Introductory example\n",
        "3. Tokenization\n",
        "4. Model Architecture\n",
        "5. Pipeline using HuggingFace  \n",
        "6. Model loading"
      ],
      "metadata": {
        "id": "kIMoNmPLWZUv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modeling Sequential Data\n",
        "\n",
        "Sequences are variable-length lists with data in subsequent iterations that depends on previous iterations (or tokens).\n",
        "\n",
        "Mathematically:\n",
        "A sequence is a list of tokens: $$T = [t_1, t_2, t_3,...,t_N]$$ where each token within the list depends on the others with a particular probability:\n",
        "\n",
        "$$P(t_2 | t_1, t_3, t_4, ..., t_N)$$"
      ],
      "metadata": {
        "id": "mXf7L02LXrzk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The purpose of sequential modeling is to learn these probabilities for possible tokens in a distribution to perform various tasks including:\n",
        "* Sequence generation based on a prompt\n",
        "* Language translation (e.g. English --> French)\n",
        "* Property prediction (predicting a property based on an entire sequence)\n",
        "* Identifying mistakes or missing elements in sequential data"
      ],
      "metadata": {
        "id": "dpeNk1umXs9r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scientific sequential data modeling examples"
      ],
      "metadata": {
        "id": "gY7pqmOiX0Sd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### Nucleic acid sequences + genomic data\n",
        "Nucleic acid sequences can be used to predict translation of proteins, mutations, and gene expression levels.\n",
        " ![RNA sequences](https://github.com/architvasan/ai_science_local/blob/main/images/RNA-codons.svg.png?raw=1)"
      ],
      "metadata": {
        "id": "zyUUYza0X4WV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iStN3PrgWQ5t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}